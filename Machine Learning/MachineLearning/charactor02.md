---
title: 概念学习和一般到特殊序
tags: 机器学习, 概念学习
---

[TOC]

# 概念学习和一般到特殊序

从特殊的训练样例中归纳一般函数是机器学习的中心问题。

**概念学习：** 给定某一类别的若干正例和反例，从中获得该类别的一般定义。概念学习也可以看作是一个搜索问题的过程，它在预定义的假设空间中搜索假设，使其与训练样例有最佳的拟合度。

多数情形下，为了高效的搜索，可以利用假设空间中一种自然形成的结构 —— **一般到特殊偏序结构**。

## 简介

许多机器学习问题涉及到从特殊训练样例中得到一般概念。每个概念可被看作一个对象或事件集合，它是从更大的集合中选取的子集，或者是在这个较大集合中定义的布尔函数。

接下来需要考虑的问题是，给定一样例集合以及每个样例是否属于某一个概念的标注，怎么自动推断出该概念的一般定义。这一问题被称为 **概念学习**，或称从样例中逼近布尔值函数。

### 概念学习

**概念学习** 是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数。

## 概念学习任务

一般来说，任何概念学习任务能被描述为：实例的集合、实例集合上的目标函数、候选假设的集合以及训练样例的集合。

### 术语定义

概念定义在一个**实例**（instance）集合之上，这个集合表示为 $X$ 。

待学习的概念或函数称为**目标概念**（target concept），记作 $c$ 。一般来说，$c$ 可以是定义在实例集 $X$ 上的任意布尔函数，即 $c: X \rightarrow {0, 1}$ 。

学习目标概念时，必须提供一套**训练样例**（training examples），每个样例为 $X$ 中的一个实例 $x$ 以及它的目标概念 $c(x)$ 。对于 $c(x) = 1$ 的实例被称为**正例**（positive example），或称为目标概念的成员。对于 $c(x)=0$ 的实例为**反例**（negative example），或称为非目标概念成员。经常可以用序偶 $<x, c(x)>$ 来描述训练样例，表示其包含了实例 $x$ 和目标概念 $c(x)$ 。符号 $D$ 用来表示训练样例的集合。

一旦给定目标概念 $c$ 的训练样例集，学习器面临的问题就是假设和设计 $c$ 。使用符号 $H$ 来表示**所有可能假设**（all possible hypotheses）的集合，这个集合才是为确定目标概念所考虑的范围。通常 $H$ 依设计者所选择的假设表示而定。 $H$ 中每个的假设 $h$ 表示 $X$ 上定义的布尔函数，即 $h: X \rightarrow{0,1}$ 。机器学习的目标就是寻找一个假设 $h$ ，使对于 $X$ 中的所有 $x$ ，$h(x)=c(x)$ 。

### 归纳学习假设

机器学习的任务是在整个实例集合 $X$ 上确定与目标概念 $c$ 相同的假设 $h$ 。

归纳学习算法最多只能保证输出的假设能与训练样例相拟合。如果没有更多信息，我们只能假设，对于未见实例最好的假设就是与训练数据最佳拟合的假设。这是归纳学习的一个基本假设。

**归纳学习机假设: ** 任一假设如果在足够大的训练样例集中很好地逼近目标函数，它也能在未见实例中很好地逼近目标函数。

## 作为搜索的概念学习

概念学习可以看作是一个搜索的过程，范围是假设的表示所隐含定义的整个空间。搜索的目标是为了寻找能最好地拟合训练样例的假设。

必须注意到，当假设的表示形式选定后，那么也就隐含地为学习算法确定了所有假设的空间。这些假设是学习程序所能表示的，也是能够学习的。

区分，实例空间和假设空间。假设集合中又区分语法不同的假设和语义不同的假设，参考书P17。

### 假设的一般到特殊序

许多概念学习算法中，搜索假设空间的方法依赖于一种针对任意概念学习都很有效的结构：假设的一般到特殊序关系。利用假设空间的这种自然结构，可以在无限的假设空间中进行彻底搜索，而不需要明确地列举所有的假设。

**概念**

**更一般**，P18。直观上的“比…更一般”的精确定义：首先，对 $X$ 中的任意实例 $x$ 和 $H$ 中的任意假设 $h$ ，我们说当且仅当 $h(x) = 1$ 时 $x$ 满足 $h$ 。 

令 $h_j$ 和 $h_k$ 为在 $X$ 上定义的布尔函数。称 $h_j\ more\_general\_than\_or\_equal\_to\ h_k$ 记作 $h_j\geq_gh_k$，当且仅当

$(\forall\ x\in X)[(h_k(x)=1)\rightarrow(h_j(x)=1)]$

考虑一假设严格地比另一假设更一般的情形，我们说 $h_j$ 严格地 $more\_general\_than h_k$ （写作 $h_j >_gh_k$），当且仅当$(h_j\geq_gh_k)\bigwedge(h_k\ngeq_gh_j)$ 。还可以定义逆向的关系“比…更特殊”为 $h_j\ more\_specific\_than\ h_k$ 当$h_k\ more\_general\_than\ h_j$。

需要注意得失， $\geq_g$ 和 $>_g$ 关系的定义独立于目标概念。它们只依赖于满足这两个假设的实例，而与根据目标概念进行的实例分类无关。用形式化的语言来说，$\geq_g$ 关系定义了假设空间 $H$ 上的一个**偏序**（即这个关系是自反、反对称和传递的）。

$\geq_g$ 关系很重要，因为它在假设空间 $H$ 上对任意概念学习问题提供了一种有效的结构。概念学习算法将利用这一偏序结构有效地搜索假设空间。

## FIND-S: 寻找极大特殊假设

如何使用 $more\_general\_than$ 偏序来搜索与训练样例相一致的假设呢？一种办法是从 $H$ 中最特殊假设开始，然后在该假设**覆盖**正例失败时将其**一般化**（当一假设能正确地划分一个正例时，称该假设覆盖该正例）。使用偏序实现的Find-S算法的精确描述如下：

---

- 将 $h$ 初始化为 $H$ 中最特殊假设
- 对每个正例 $x$ 
  - 对 $h$ 的每个属性约束 $a_i$
    - 如果 $x$ 满足 $a_i$ ，那么不做任何处理
    - 否则将 $h$ 中 $a_i$ 替换为 $x$ 满足的下一个更一般约束
- 输出假设 $h$

---

Find-S算法简单地忽略每一个反例。一般情况下，只要我们假定假设空间 $H$ 确实包含真正的目标概念 $c$ ，而且训练样例不包含错误，那么当前的假设 $h$ 不需要因反例的出现而更改。原因在于当前假设 $h$ 是 $H$ 中与所观察到的正例相一致的最特殊的假设，由于假定目标概念 $c$ 在 $H$ 中，而且它一定是与所有正例一致的，那么 $c$ 一定比 $h$ 更一般，而且目标概念 $c$ 不会覆盖一个反例，因此 $h$ 也不会。因此，对反例，$h$ 不需要作出任何更改。

Find-S 算法演示了一种利用 $more\_general\_than$ 偏序来搜索假设空间的方法。这一搜索沿着偏序链，从较特殊的假设逐渐转移到较一般的假设。

在每一步，假设只在需要覆盖新的正例时被一般化。因此，每一步得到的假设都是在那一点上与训练样例一致的最特殊的假设。

Find-S算法的重要特点是：对以属性约束的合取式描述的假设空间，Find-S保证输出为 $H$ 中与正例一致的最特殊的假设。只要正确的目标概念包含在 $H$ 中，并且训练数据都是正确的，最终的假设也与所有反例一致。

## 变型空间和候选消除算法

概念学习的另一种途径即**候选消除**（CANDIDATE-ELIMINATION）算法。它们解决FIND-S中的若干不足之处。FIND-S输出的假设只是 $H$ 中能够拟合训练样例的多个假设中的一个。而在候选消除算法中，输出的是与训练样例一致的所有假设的集合。令人惊讶的是，候选消除算法在描述这一集合时不需要明确列举其所有成员。这一归功于  $moew\_general\_than$ 偏序结构。在这里需要维护一个一致假设集合的简洁表示，然后在遇到新的训练样例时逐步精化这一表示。

然而，候选消除算法和FIND-S算法的实际应用都受到限制，因为它们在训练数据含有噪声时性能较差。

### 表示

候选消除算法寻找与训练样例一致的所有假设。首先，当一个假设能正确分类一组样例时，我们称这个假设是与这些样例**一致**（consistent）的。

定义：一个假设 $h$ 与训练样例集合 $D$ 一致，当且仅当对 $D$ 中每一个样例 $<x,c(x)>$ 都有 $h(x)=c(x)$ 。

$Consistent(h, D)\equiv(\forall<x,c(x)>\in D)\ h(x)=c(x)$

