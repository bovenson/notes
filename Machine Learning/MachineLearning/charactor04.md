---
title: 人工神经网络
tags: 机器学习, 人工神经网络
---

# 人工神经网络

人工神经网络(Artificial Neural Networks, ANN) 提供了一种普遍而且实用的方法从样例中学习值为实数、离散值或向量的函数。像反向传播(BACKPROPAGATION)这样的算法，实用梯度下降来调节网络参数以最佳拟合由输入-输出对组成的训练集合。ANN学习对于训练数据中的错误健壮性很好。

## 简介

神经网络学习方法对于逼近实数值、离散值和向量值得目标函数提供了一种健壮性很强的方法。

人工神经网络是由一系列简单的单元相互密集连接构成的，其中每一个单元有一定数量的实值输入（可能是其他单元的输出），并产生单一的实数值输出（可能成为其它很多单元的输入）。

ANN系统的一个动机就是获取基于分布表示的高度并行算法。

反向传播算法假定网络是一个固定结构，对应一个有向图，可能包含环。ANN学习就是为图中的每一条边选取权值。

## 适合神经网络学习的问题

ANN学习非常适合于这样的问题：训练集合为含有噪声的复杂传感器数据。它也适用于需要许多符号表示的问题。反向传播是最常用的ANN学习技术。它适合具有以下特征的问题：

- 实例是用很多属性-值对表示：要学习的目标函数是定义在可以用向量描述的实例之上的，向量由预先定义的特征组成。
- 目标函数的输出可能是离散值、实数值或者由若干实数属性或离散属性组成的向量。
- 训练数据可能包含错误：ANN学习算法对于训练数据中的错误有非常好的健壮性
- 可容忍长时间的训练
- 可能需要快速求出函数值
- 人类能否理解学到的目标函数是不重要的

组成神经网络的几种主要单元：感知器（perceptron）、线性单元（linear unit）和sigmoid单元（sigmoid unit）。

## 感知器

一种类型的ANN系统是以被称为感知器的单元为基础的。感知器以一个实数值向量作为输入，计算这些输入的线性组合，然后如果结果大于某个阈值，就输出1，否则输出-1。更精确地，如果输入为 $x_1$ 到 $x_n$ ，那么感知器计算的输出为：

$ o(x_1, …, x_n) = \begin{cases} 1 & if\ w_0+w_1x_1+w_2x_2+…+w_nx_n > 0 \\ -1 & otherwise \end{cases} $

其中每一个 $w_i$ 是一个实数常量，或叫做权值，用来决定输入 $x_i$ 对感知器输出的贡献率。数量（$-x_0$）是一个阈值，它是为了使感知器输出1，输入的加权和 $w_1x_1+w_2x_2+…+w_nx_n$ 必须超过的阈值。

有时会把感知器函数写为：

$o(\vec{x}) = sgn(\vec{w} \cdot \vec{x})$

其中

$sng(y) = \begin{cases} 1 & if\ y > 0 \\ -1 & otherwise \end{cases}$

学习一个感知器意味着选择权 $w_0, … , w_n$ 的值。所以感知器学习要考虑的候选假设空间 $H$ 就是所有可能的实数值权向量的集合。

$H = \{  \vec{w} | \vec{w} \in \Re ^{(n+1)} \}$

### 感知器的表征能力

可以把感知器看作是 n 维实例空间（即点空间）中的超平面决策面。对于超平面一侧的实例，感知器输出1，对于另一侧的实例输出-1。某些正反样例集合不可能被任一超平面分割，那些可以被分割的称为线性可分样例集合。

### 感知器训练法则

从如何学习单个感知器的权值开始。准确地说，这里的学习任务是决定一个权向量，它可以使感知器对于给定的训练样例输出正确的1或-1。解决这个学习任务的算法有感知器法则和delta法则等。这两种算法保证收敛到可接受的假设。这两种方法对于ANN很重要，因为它们提供了学习多个单元构成的网络的基础。

为得到可接受的权向量，一种办法是从随机的权值开始，然后反复地应用这个感知器到每个训练样例，只要它误分类样例就修改感知器的权值。重复这个过程，知道感知器正确分类所有的训练样例。每一步根据感知器训练法则来修改权值，也就是修改与输入 $x_i$ 对应的权 $w_i$ ，法则如下：

$w_i \leftarrow w_i + \Delta w_i$

其中：

$\Delta w_i = \eta (t - o) x_i$

这里 t 是当前训练样例的目标输出，o 是感知器的输出，$\eta$ 是一个正的常数称为学习速率。学习速率的作用是缓和每一步调整权的程度。它通常被设为一个小的数值（如：0.1），而且有时会使其随着权调整次数的增加而衰减。

可以证明，在有限次地使用感知器训练法则后，训练过程会收敛到一个能正确分类所有训练样例的权向量，前提是训练样例线性可分，并且使用了充分小的 $\eta$ 。如果数据不是线性可分的，那么不能保证训练过程收敛。

### 梯度下降和delta法则

如果训练样本不是线性可分的，那么delta法则会收敛到目标概念的最佳近似。

delta法则的关键思想是使用梯度下降来搜索可能的权向量的假设空间，以找到最佳拟合训练样例的权向量。

delta法则的重要性在于：

- 它为反向传播算法提供了基础，而反向传播算法能够学习多个单元的互连网络
- 对于包含多种不同类型的连续参数化假设的假设空间，梯度下降是必须遍历这样的假设空间的所有学习算法的基础

最好把delta训练法则理解为训练一个无阈值的感知器，也就是一个线性单元，它的输出 o 如下：

$o(\vec{x})  = \vec{w} \cdot \vec{x}$

为了推导线性单元的权值学习法则，先指定一个度量标准来衡量假设（权向量）相对于训练样例的训练误差：

$E(\vec{w})  \equiv \frac{1}{2} \sum \limits _{d \in D} (t_d - o_d) ^2$

其中，D 是训练样例集合， $t_d$ 是训练样例 $d$ 的目标输出，$o_d$ 是线性单元对训练样例 $d$ 的输出。

#### 可视化假设空间

为了确定一个使 E 最小化的权向量，梯度下降搜索从一个任意的初始权向量开始，然后以很小的步伐反复修改这个向量。每一步都沿误差曲面产生最陡峭下降的方向修改权向量，继续这个过程知道得到全局的最小误差点。

#### 梯度下降法则的推导

通过计算 $E$ 相对向量 $\vec{w}$ 的每个分量的导数来得到这个方向。这个向量导数被称为 $E$ 对于 $\vec{w}$ 的**梯度**，记作 $\nabla E(\vec{w}) \equiv [ \, \frac {\partial E}{\partial w_0},\frac {\partial E}{\partial w_1},…,\frac {\partial E}{\partial w_n}  ] \,$

$\nabla E(\vec{w})$ 本身是一个向量，它的成员时 $E$ 对每个 $w_i$ 的偏导数。当梯度被解释为权空间的一个向量时，它确定了使 $E$ 最陡峭上升的方向。所以，这个方向的反方向给出了最陡峭下降的方向。那么梯度下降的训练法则是：

$\vec {w} \leftarrow \vec{w} + \Delta \vec{w}$

其中：

$\Delta \vec w = - \eta \nabla E (\vec w)$

 这里 $\eta$ 是一个正的常数叫学习速率，它决定梯度下降搜索中的步长。这个训练法则也可以写成它的分量形式：

$w_i = w_i + \Delta w_i$

其中：

$\Delta w_i = - \eta \frac{\partial E}{\partial w_i}$

最陡峭的下降可以按照比例 $\frac{\partial E}{\partial w_i}$ 改变 $\vec{w}$ 中的每一个分量 $w_i$ 来实现。

我们需要一个高效的方法，在每一步都计算这个梯度：从训练误差公式($E(\vec{w})  \equiv \frac{1}{2} \sum \limits _{d \in D} (t_d - o_d) ^2$)中计算 $E$ 的微分，从而得到组成这个梯度向量的分量 $\frac{\partial \vec{E}}{\partial w_i}$ ：

$\frac{\partial \vec{E}}{\partial w_i} = \sum \limits _{d \in D} (t_d-o_d)(-x_{id})$

其中，$x_{id}$ 表示训练样例 $d$ 的一个输入分量 $x_i$ 。

梯度下降权值更新法则：

$\Delta w_i = \eta \sum \limits _{d \in D} (t_d - o_d) x_{id}$

