---
title: 最小二乘
tags:
	- 数学
categroies:
	- 数学
---

# 最小二乘法



-   **本质**是最小化系数矩阵所张成的向量空间到观测向量的欧式误差距离

-   一种**常见的描述**是残差满足正态分布的最大似然估计

-   **主要思想**

    -   就是选择未知参数，使得理论值与观测值之差的平方和达到最小

    -   基本形式

    -   $$
        H = \sum \limits_{i=1}^{m} (y-y_i)^2	\\
        其中 \hspace{1em} y = f(x)
        $$

    -   也即，如何选取参数 $$x$$ 使得 $$H$$ 最小

-   1829年，高斯提供了最小二乘法的优化效果强于其他方法的证明，见高斯-马尔可夫定理

-   最小二乘法（又称最小平方法）是一种数学优化技术

-   它通过最小化误差的平方和寻找数据的最佳函数匹配

-   利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小

-   最小二乘法还可用于曲线拟合

-   其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达

-   简而言之，最小二乘法同梯度下降类似，都是一种**求解无约束最优化问题的常用方法**，并且**也可以用于曲线拟合，来解决回归问题**。**最小二乘法实质就是最小化“均方误差”**，而均方误差就是残差平方和的$$\frac{1}{m}$$(m为样本数)，同时均方误差也是回归任务中最常用的性能度量

-   正规方程的解法就是最小二乘法求解析解的解法

-   通过一次计算，直接求出参数

## 优缺点

**优点**

-   简单易实现

**缺点**

-   对噪声的容忍度很低
    -   对于噪声的处理，有有加权最小二乘等方法等

## 推理

**问题引述**

>   现在大家都越来越重视自己的身体健康。现代人最常见的亚健康问题就是肥胖，本博主身体棒棒哒，唯一困扰本博主的健康问题就是超重。（好吧，承认自己是个死胖子就完了） 
>   假设身高是变量X，体重是变量Y，我们都知道身高与体重有比较直接的关系。生活经验告诉我们：一般身高比较高的人，体重也会比较大。但是这只是我们直观的感受，只是很粗略的定性的分析。在数学世界里，我们大部分时候需要进行严格的定量计算：能不能根据一个人的身高，通过一个式子就能计算出他或者她的标准体重？ 
>   接下来，我们肯定会找一堆人进行采用（请允许我把各位当成一个样本）。采样的数据，自然就是各位的身高与体重。（为了方便计算与说明，请允许我只对男生采样）经过采样以后，我们肯定会得到一堆数据(x1,y1),(x2,y2),⋯,(xn,yn)(x1,y1),(x2,y2),⋯,(xn,yn)，其中x是身高，y是体重。 
>   得到这堆数据以后，接下来肯定是要处理这堆数据了。生活常识告诉我们：身高与体重是一个近似的线性关系，用最简单的数学语言来描述就是y=β0+β1xy=β0+β1x。于是，接下来的任务就变成了：怎么根据我们现在得到的采样数据，求出这个β0β0与β1β1呢？

# 狭义广义论

## 狭义的最小二乘

指的是在线性回归下采用最小二乘准则（或者说叫做最小平方），进行线性拟合参数求解的、矩阵形式的公式方法。所以，这里的「最小二乘法」应叫做**「最小二乘算法」或者「最小二乘方法」**百度百科「最小二乘法」词条中对应的英文为「The least square method」。

基于线性回归，有两个细节比较重要：

-   第一，线性回归的模型假设，这是最小二乘方法的优越性前提，否则不能推出最小二乘是最佳（即方差最小）的无偏估计，具体请参考高斯-马尔科夫定理。特别地，当随机噪声服从正态分布时，最小二乘与最大似然等价。
-   第二，由于是线性回归/拟合，因此可以很容易的求出**全局最优的闭式解** **close form solution**，也即我们通常看到的那几个矩阵形式，给了input data可以一步到位算拟合参数，而不是像梯度下降法或者牛顿法那样一点点地迭代优化调参最后到达极值点。

## 广义的最小二乘

指的是上文提到过的最小二乘准则，本质上是一种evaluation rule或者说objective funcion，这里的「最小二乘法」应叫做**「最小二乘法则」或者「最小二乘准则」，英文可呼为** **LSE（least square error）。**

举个例子，我要优化一个深度神经网络DNN（Deep neural network）的网络参数（换言之，优化此网络对于已知数据拟合结果的正确性），可不可以用最小二乘准则去衡量某一拟合结果相对于标准答案的偏差程度呢？可以。而同时，由于DNN模型本身的复杂性，我们没有办法像线性拟合时那样，在理论和公式的层面求出一个close form solution，因此需要引入所谓的BP算法（实质上就是梯度下降法）进行参数的迭代求解。

# 参考

-   [参考一](http://blog.csdn.net/bitcarmanlee/article/details/51589143)
-   [参考二](https://www.cnblogs.com/wangkundentisy/p/7505487.html)