---
title: 线性模型
tags:
	- 机器学习
	- 线性模型
categories:
	- 机器学习
---

# 基本形式

线性模型（linear model）试图通过属性的线性组合来进行预测的函数，即：
$$
f(x) = \omega_1 x_1 + \omega_2x_2 + \omega_3x_3 + ... + \omega_dx_d + b
$$

# 线性回归

线性回归（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。

-   均方误差

    -   是回归任务中最常用的**性能度量**
    -   几何意义：对应了常用的**欧几里得距离**或简称**欧氏距离**

-   最小二乘法

    -   基于均方误差最小化来进行模型求解的方法称为**最小二乘法**
    -   线性回归中，最小二乘法就是试图找到一条曲线，使所有样本到直线上的欧氏距离之和最小

-   对数线性回归

    -   $$
        \ln y = \omega ^T x + b
        $$

        或
        $$
        y = e ^ {w^Tx+b}
        $$

    -   **对数线性回归** 实际上是在试图让 $$e^{\omega ^T x + b}$$ 逼近 $$y$$

    -   形式上仍是线性回归，但实质上是在求取输入空间到输出空间的非线性函数映射

    -   这里的对数函数起到了将线性回归模型的预测值与真实标记联系起来的作用

-   广义线性模型

    -   对数线性回归是广义模型的特例

    -   考虑单调可微函数 $$g(\cdot)$$，令
        $$
        y = g^{-1} (\omega ^T x + b)
        $$

    -   函数 $$g(\cdot)$$ 称为**联系函数**

# 对数几率回归

## 对数几率函数

$$
y = \frac{1}{1 + e ^ {-z}}
$$

对数几率函数是一种 **Sigmoid函数**，将 $$z$$ 值转化为一个接近0或1的 $$y$$ 值。

将对数几率函数作为 $$g^- (\cdot)$$ 代入$$y = g^{-1} (\omega ^T x + b)$$ ，得到：
$$
y = \frac{1}{1 + e ^{-(\omega ^ T x + b)}}
$$
类似于 $$\ln y = \omega ^T x + b$$ ，可变化为
$$
\ln {\frac{y}{1-y}} = \omega^T x + b
$$

### 几率

若将 $$y$$ 视为样本 $$x$$ 作为正例的可能性，则 $$1-y$$ 是其反例可能性，两者的比值
$$
\frac{y}{1-y}
$$
称为 **几率(odds)**。**反映了 $$x$$ 作为正例的相对可能性**。对几率取对数则得到 **对数几率** 
$$
\ln{\frac{y}{1-y}}
$$

-   可知，公式 $$y = \frac{1}{1 + e ^{-(\omega ^ T x + b)}}$$ 实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为 **对数几率回归**
-   名字是“回归”，但实际却是一种分类学习方法，优点：
    -   直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题
    -   不仅预测出“类别”，而是可得到近似概率预测，对许多需利用概率辅助决策的任务很有用
    -   对率函数是任意阶可导的凸函数，有很好的数学性质

# 线性判别分析

线性判别分析（Linear Discriminat Analysis，LDA）是一种经典的线性学习方法。

思想：

-   给定训练样例集，设法将样例投影到一条直线上
-   使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离
-   在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别

# 多分类学习

-   利用二分类学习器来解决多分类问题
-   多分类学习的基本思路是 **拆解法**， 将多分类任务拆为若干个二分类任务求解：
    -   先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器
    -   在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果

## 拆分策略

-   一对一
    -   将N个类别两两配对，产生 $$N(N-1)/2$$ 个二分类任务
-   一对其余
    -   N个训练样例，将一个类的样例作为正例，所有其他类的样例作为反例
    -   **比较：** 
        -   OvO 储存开销和测试时间开销通常比 OvR 更大
        -   OvO 的训练时间开销通常比 OvR 更小
        -   性能相当
-   多对多

# 类别不平衡问题

-   类别不平衡问题指分类任务中不同类别的训练样例数目差别很大的情况
-   观测几率：$$\frac{m^+}{m^-} = \frac{正例数据}{反例数目}$$
-   分类器的预测几率高于观测几率就应判定为正例
    -   $$\frac{y'}{1-y'} = \frac{y}{1-y} * \frac{m^+}{m^-}$$
-   针对类别不平衡一个基本策略是 **再缩放**
    -   对反类样例进行 **欠采样**，使正负样例数目相近
    -   对正类样例进行 **过采样**，使正负样例数目相近
    -   直接基于原始训练集进行学习，但是将公式$$\frac{y'}{1-y'} = \frac{y}{1-y} * \frac{m^+}{m^-}$$ 嵌入决策过程，称为 **阈值移动**