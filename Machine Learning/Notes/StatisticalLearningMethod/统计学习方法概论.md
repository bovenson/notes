---
title: 统计学习方法概论
tags:
	- 统计学习方法
categories:
	- 统计学习方法
---

# 统计学习

如果一个系统能够通过执行某个过程改进它的性能，这就是**学习**。

**统计学习的对象**

- 统计学习的对象是**数据**。
- 它从数据出发，提取数据的**特征**，抽象出数据的**模型**，发现数据中的**知识**，又回到对数据的**分析和预测**中去。
- 统计学习关于数据的**基本假设**是同类数据具有一定的统计规律性，这是统计学习的**前提**。
- 可以用**随机变量**描述数据中的**特征**，用**概率分布**描述数据的**统计规律**。

**统计学习的目的**

- 统计学习的总目标是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析。
- 对数据的预测与分析是通过构建概率统计模型实现的。

**统计学习的方法**

- 统计学习的方法是基于数据构建统计模型，从而对数据进行预测与分析。
- 统计学习包括
  - 监督学习
  - 非监督学习
  - 半监督学习
  - 强化学习
- 统计学习方法的步骤
  - 得到一个有限的**训练数据集合**（假设数据是独立同分布产生的）
  - 确定包含所有可能的模型的假设空间，即**学习模型的集合**
  - 确定模型选择的准则，即**学习的策略**
  - 实现求解最优模型的算法，即**学习的算法**
  - 通过学习方法**选择最优模型**
  - 利用学习的最优模型**对新数据进行预测或分析**

# 监督学习

- 监督学习（supervised learning）的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。

## 基本概念

**输入空间**

所有输入可能的集合。

**输出空间**

所有输出可能的集合。

**特征空间**

每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示。这时，所有特征向量存在的空间称为特征空间（feature space）。

特征空间的每一维对应一个特征。

**联合概率分布**

- 监督学习假设输入与输出的随机变量 $X$ 和 $Y$ 遵循联合概率分布 $P(X,Y)$。
- $P(X,Y)$ 表示分布函数，或分布密度函数。
- 训练数据与测试数据被看作是依联合概率分布 $P(X,Y)$ 独立同分布产生的。

**假设空间**

- 监督学习的目的在于学习一个由输入到输出的映射。
- 模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）。
- 假设空间的确定意味着学习范围的确定。
- 监督学习的模型可以是
  - 条件概率分布 $P(Y|X)$
  - 决策函数 $y = f(x)$
- 对具体的输入进行相应的输出预测时，写作 $P(y|x) 或 y = f(x)$。

## 问题的形式化

监督学习利用训练数据集学习一个模型，再用模型对测试样本集进行预测（prediction）。

监督学习分为学习和预测两个过程，分别由学习系统和预测系统完成。

**学习系统**

- 利用给定的训练数据集
- 通过学习（或训练）得到一个模型
- 该模型表示为条件概率分布 $\hat{P}(Y|X)$ 或决策函数 $Y = \hat{f}(X)$，描述的是输入和输出随机变量之间的映射关系

**预测系统**

- 对于测试样本集中的输入 $x_{N+1}$，由模型 $y_{N+1} = \operatorname*{arg max}\limits_{y_{N+1}} \hat{P}(y_{N+1}|x_{N+1}) \hspace{1em}或\hspace{1em} y_{N+1} = \hat{f}(x_{N+1}) $

# 统计学习三要素

统计学习方法由模型、策略、算法构成。

## 模型

统计学习首要考虑的问题是学习什么样的模型

**假设空间**

- 假设空间 $\mathcal{F}$ 可以定义为决策函数或条件概率分布的集合

- $$
  \mathcal{F} = \{f|Y=f(X)\}	\hspace{2em} 决策函数	\\ 或 \\
  \mathcal{F} = \{P|P(Y|X)\}	\hspace{2em} 条件概率分布
  $$

  - 其中，$X、Y$ 是定义在输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$ 上的变量，这是 $\mathcal{F}$ 通常是由一个参数向量决定的函数族

  $$
  \mathcal{F} = \{ f|Y = f_\theta (X), \theta \in \R^{n} \} \hspace{2em} \\ 或 \\
  \mathcal{F} = \{ P|P_\theta (Y|X), \theta \in \R^n \}
  $$

  - 参数向量 $\theta$ 取值于 $n$ 维欧氏空间 $\R^n$，称为**参数空间** （parameter space）

## 策略

有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的目标在于从假设空间中选取最优模型。

- 首先引入损失函数与风险函数的概念。
  - 损失函数度量模型的**一次预测**的好坏
  - 风险函数度量**平均意义**下模型预测的好坏

- **损失函数**

  - 常用损失函数

    - 0-1损失函数

    $$
    L(Y,f(X)) = \begin{equation}
      \left\{
       \begin{aligned}
       1,\hspace{1em} Y \neq f(X)  \\
       0, \hspace{1em} Y = f(X)\\
       \end{aligned}
       \right.
      \end{equation}
    $$

    - 平方损失函数

  - $$
    L(Y,f(X)) = (Y - f(X))^2
    $$

    - 绝对损失函数

  - $$
    L(Y,f(X)) = |Y - f(X)|
    $$

    - 对数损失函数或对数似然损失函数

    $$
    L(Y,P(Y|X)) = - \log P(Y|X)	
    $$




- **期望损失或风险函数**

$$
R_{\exp}(f) = E_p[L(Y,f(X)] = \int_{\mathcal{X}\times\mathcal{Y}} L(y,f(x))P(x,y) \mathrm{d}x\mathrm{d}y
$$
- **经验损失或经验风险**

$$
R_{\operatorname*{emq}}(f) = \frac{1}{N} \sum\limits_{i=1}^{N} L(y_i,f(x_i))
$$

- 期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本集的平均损失。期望风险不能直接计算，常用经验风险估计期望风险。

- **经验风险最小化**

  - 经验风险最小化策略认为，经验风险最小的模型是最优的模型
    $$
    \min\limits_{f\in\mathcal{F}} \frac{1}{N} \sum\limits_{i=1}^{N} L(y_i,f(x_i))
    $$

  - 当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计

  - 当样本容量很小时，经验风险最小化学习的效果未必很好，会产生 **过拟合** 现象

- **结构风险最小化**

  - 结构风险最小化是为了防止过拟合提出的策略

  - 结构风险最小化等价于**正则化**

  - 结构风险在经验风险上加上表示模型复杂度的正则化项或惩罚项
    $$
    R_{\operatorname*{srm}}(f) = \frac{1}{N} \sum\limits_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
    $$

  - 结构风险最小化策略认为，结构风险最小的模型是最优的模型

- 结合经验风险最小化和结构风险最小化，监督学习就变成了经验风险或结构风险函数的最优化问题
  $$
  \min\limits_{f\in\mathcal{F}} \frac{1}{N}\sum\limits_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
  $$


## 算法

- 最后需要考虑的是用什么样的计算方法求解最优模型。这时，统计学习问题**归结为最优化问题**。


- 最优化问题有显式的**解析解**时，相对简单。但通常解析解不存在，就需要用**数值计算**的方法求解。
- 如何找到全局最优解，并使求解过程非常高效，就成了一个重要问题。