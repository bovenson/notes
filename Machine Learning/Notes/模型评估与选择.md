---
title: 模型评估与选择
tags: 
	- 机器学习
	- 模型评估
categories:
	- 机器学习
---

[TOC]

# 经验误差与过拟合

- **错误率**：把分类错误的样本数占样本总数的比例称为“错误率”
- **精度**：精度 = 1 - 错误率
- **误差**：学习器的实际预测输出与样本的真实输出之间的差异称为“误差”
- **训练误差或经验误差**：学习器在训练集上的误差
- **泛化误差**：在新样本上的误差
- **过拟合**：把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样会导致泛化性能下降
- **欠拟合**：指对训练样本的一般性质尚未学好

# 评估方法

## 留出法

将数据集 $$D$$ 划分成两个互斥集合

-   训练集 $$S$$
-   测试集 $$T$$

## 交叉验证法

将数据集  $$D$$ 划分成 $$k$$ 个大小相似的互斥子集。每个子集 $$D_i$$ 都尽可能保持数据分布的一致性。

-   每次用 $$k-1$$ 个子集的并集作为训练集，余下的那个子集作为测试集
-   得到 $$k$$ 组训练/测试集，从而进行 $$k$$ 次训练和测试
-   最终返回 $$k$$ 个测试结果的均值

## 自助法

直接以自助采样法为基础。

给定样本数据集 $$D$$ 进行采样产生数据集 $$D'$$ ：

-   每次随机从 $$D$$ 中挑选一个样本，将其拷贝放入 $$D'$$
-   将该样本放入初始数据集 $$D$$ 中，使其下次还有可能会被抽中
-   重复 m 次后，得到数据集 $$D'$$

初始数据集 $$D$$ 中约有 36.8% 的样本未出现在采样数据 $$D'$$ 中。

**优点**

-   数据集较小、难以有效划分训练/测试集时，很有用
-   可以从初始数据集中产生多个不同的训练集

**缺点**

-   产生的数据集改变了初始数据集的分布，这回引入估计偏差

# 性能度量

对学习器的泛化性能进行评估。

-   回归任务中最常用的性能度量是 **均方误差**

## 错误率与精度

-   $$错误率 = \frac{分类错误样本数}{样本总数}$$
-   $$精度 = \frac{分类正确的样本数}{样本总数}$$

## 查准率、查全率与F1

| 真实结果\预测结果 | 正例                       | 反例                       |
| ----------------- | -------------------------- | -------------------------- |
| 正例              | TP(真正例，True Positive)  | FN(假反例，False Negative) |
| 反例              | FP(假正例，False Positive) | TN(真反例，True Negative)  |

-   查准率：$$P =  \frac{TP}{TP+FP}$$
-   查全率：$$P =  \frac{TP}{TP+FN}$$
-   F1：$$F1 = \frac{2*P*R}{P+R} = \frac{2*TP}{样例总数+TP-TN}$$
-   一般来说，查准率高时，查全率往往偏低；查全率高时，查准率往往偏低。

## ROC与AUC

-   可根据任务需求选择不同的截断点（分类阈值(threshold)）
-   重视“查准率”，选择排序靠前的位置进行截断
-   重视“查全率”，选择排序靠后的位置进行截断
-   排序本身的质量好坏，体现了综合考虑学习器在不同任务下的“期望泛化性能”的好坏
-   ROC曲线是从这个角度研究学习器泛化性能的有力工具
-   ROC全称是“受试者工作特征”
-   ROC曲线的纵轴是“真正例率”；横轴是“假正例率”
    -   真正例率：$$TPR = \frac{TP}{TP + FN}$$
    -   假正例率：$$FPR = \frac{FP}{TN+FP}$$
-   AUC：Area Under ROC Curve，ROC曲线下面的面积
-   AUC考虑的是样本预测的排序质量，与排序误差有紧密关系

## 代价敏感错误率与代价曲线

-   为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”
-   在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化“总体代价”
-   代价曲线直接反映出学习器的期望总体代价，ROC曲线不能

# 比较检验

机器学习中性能比较几个重要因素：

-   希望比较的是**泛化性能**
-   测试集上的性能与测试集本身的选择有很大关系
-   很多机器学习算法本身有一定的随机性

**统计假设检验**（hypothesis test）为我们进行机器学习性能比较提供了重要依据。基于假设检验结果，我们可推断出：**在测试集上观察到学习器A比B好，则A的泛化性能是否在统计意义上由于B，以及这个结论的把握有多大。**

## 假设检验

-   假设检验中的“检验”是对学习器泛化错误率分布的某种判断或猜想
-   泛化错误率与测试错误率未必相同，但相似的概率大，可根据测试错误率估推出泛化错误率的分布
-   可以使用二项检验来对假设进行检验

## 交叉验证 t 检验

-   k折交叉验证“成对 t 检验”
-   基本思想：若两个学习器性能相同，则它们使用相同的训练 / 测试集得到的测试错误率应相同
-   对“学习器A和B性能相同”这个假设做 t 检验

## McNemar 检验

-   假设是两个学习器性能相同
-   McNumber 检验考虑变量 $$\tau_{\chi^2}$$ 服从自由度为1的 $$\chi^2$$ 分布，即标准正态分布变量的平方
-   当变量值小于临界值 $$\chi _{\alpha} ^2$$ 时，不能拒绝假设，即认为两学习器的性能没有显著差别

# 偏差与方差

偏差-方差分解试图对学习算法的期望泛化错误率进行拆解。

期望输出与真实标记的差别称为偏差。

泛化误差可分解为偏差、方差与噪声之和，即：
$$
E(f;D) = bias^2(x) + var(x) + \epsilon ^2
$$

-   **偏差**：
    -   度量了学习算法的期望预测与真实结果的偏离程度
    -   刻画了学习算法本身的拟合能力
-   **方差**：
    -   度量了同样大小的训练集的变动所导致的学习性能的变化
    -   刻画了数据扰动所造成的影响
-   **噪声**：
    -   表达了当前任务上任何学习算法所能达到的期望泛化误差的下界
    -   刻画了学习问题本身的难度

一般来说，偏差与方差有冲突：

-   **偏差主导泛化错误率**：训练不足时，学习器的拟合能力不够强，训练数据的扰动**不足以**使学习器产生显著变化
-   **方差主导泛化错误率**：随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习期学到
-   **过拟合**：在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合

# 参考

-   周志华著《机器学习》