---
title: 统计学习方法概论
tags:
	- 统计学习方法
categories:
	- 统计学习方法
---

# 统计学习

如果一个系统能够通过执行某个过程改进它的性能，这就是**学习**。

**统计学习的对象**

- 统计学习的对象是**数据**。
- 它从数据出发，提取数据的**特征**，抽象出数据的**模型**，发现数据中的**知识**，又回到对数据的**分析和预测**中去。
- 统计学习关于数据的**基本假设**是同类数据具有一定的统计规律性，这是统计学习的**前提**。
- 可以用**随机变量**描述数据中的**特征**，用**概率分布**描述数据的**统计规律**。

**统计学习的目的**

- 统计学习的总目标是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析。
- 对数据的预测与分析是通过构建概率统计模型实现的。

**统计学习的方法**

- 统计学习的方法是基于数据构建统计模型，从而对数据进行预测与分析。
- 统计学习包括
  - 监督学习
  - 非监督学习
  - 半监督学习
  - 强化学习
- 统计学习方法的步骤
  - 得到一个有限的**训练数据集合**（假设数据是独立同分布产生的）
  - 确定包含所有可能的模型的假设空间，即**学习模型的集合**
  - 确定模型选择的准则，即**学习的策略**
  - 实现求解最优模型的算法，即**学习的算法**
  - 通过学习方法**选择最优模型**
  - 利用学习的最优模型**对新数据进行预测或分析**

# 监督学习

- 监督学习（supervised learning）的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。

## 基本概念

**输入空间**

所有输入可能的集合。

**输出空间**

所有输出可能的集合。

**特征空间**

每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示。这时，所有特征向量存在的空间称为特征空间（feature space）。

特征空间的每一维对应一个特征。

**联合概率分布**

- 监督学习假设输入与输出的随机变量 $X$ 和 $Y$ 遵循联合概率分布 $P(X,Y)$。
- $P(X,Y)$ 表示分布函数，或分布密度函数。
- 训练数据与测试数据被看作是依联合概率分布 $P(X,Y)$ 独立同分布产生的。

**假设空间**

- 监督学习的目的在于学习一个由输入到输出的映射。
- 模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）。
- 假设空间的确定意味着学习范围的确定。
- 监督学习的模型可以是
  - 条件概率分布 $P(Y|X)$
  - 决策函数 $y = f(x)$
- 对具体的输入进行相应的输出预测时，写作 $P(y|x) 或 y = f(x)$。

## 问题的形式化

监督学习利用训练数据集学习一个模型，再用模型对测试样本集进行预测（prediction）。

监督学习分为学习和预测两个过程，分别由学习系统和预测系统完成。

**学习系统**

- 利用给定的训练数据集
- 通过学习（或训练）得到一个模型
- 该模型表示为条件概率分布 $\hat{P}(Y|X)$ 或决策函数 $Y = \hat{f}(X)$，描述的是输入和输出随机变量之间的映射关系

**预测系统**

- 对于测试样本集中的输入 $x_{N+1}$，由模型 $y_{N+1} = \operatorname*{arg max}\limits_{y_{N+1}} \hat{P}(y_{N+1}|x_{N+1}) \hspace{1em}或\hspace{1em} y_{N+1} = \hat{f}(x_{N+1}) $

# 统计学习三要素

统计学习方法由模型、策略、算法构成。

## 模型

统计学习首要考虑的问题是学习什么样的模型

**假设空间**

- 假设空间 $\mathcal{F}$ 可以定义为决策函数或条件概率分布的集合

- $$
  \mathcal{F} = \{f|Y=f(X)\}	\hspace{2em} 决策函数	\\ 或 \\
  \mathcal{F} = \{P|P(Y|X)\}	\hspace{2em} 条件概率分布
  $$

  - 其中，$X、Y$ 是定义在输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$ 上的变量，这是 $\mathcal{F}$ 通常是由一个参数向量决定的函数族

  $$
  \mathcal{F} = \{ f|Y = f_\theta (X), \theta \in \R^{n} \} \hspace{2em} \\ 或 \\
  \mathcal{F} = \{ P|P_\theta (Y|X), \theta \in \R^n \}
  $$

  - 参数向量 $\theta$ 取值于 $n$ 维欧氏空间 $\R^n$，称为**参数空间** （parameter space）

## 策略

有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的目标在于从假设空间中选取最优模型。

- 首先引入损失函数与风险函数的概念。
  - 损失函数度量模型的**一次预测**的好坏
  - 风险函数度量**平均意义**下模型预测的好坏

- **损失函数**

  - 常用损失函数

    - 0-1损失函数

    $$
    L(Y,f(X)) = \begin{equation}
      \left\{
       \begin{aligned}
       1,\hspace{1em} Y \neq f(X)  \\
       0, \hspace{1em} Y = f(X)\\
       \end{aligned}
       \right.
      \end{equation}
    $$

    - 平方损失函数

  - $$
    L(Y,f(X)) = (Y - f(X))^2
    $$

    - 绝对损失函数

  - $$
    L(Y,f(X)) = |Y - f(X)|
    $$

    - 对数损失函数或对数似然损失函数

    $$
    L(Y,P(Y|X)) = - \log P(Y|X)	
    $$







- **期望损失或风险函数**

$$
R_{\exp}(f) = E_p[L(Y,f(X)] = \int_{\mathcal{X}\times\mathcal{Y}} L(y,f(x))P(x,y) \mathrm{d}x\mathrm{d}y
$$
- **经验损失或经验风险**

$$
R_{\operatorname*{emq}}(f) = \frac{1}{N} \sum\limits_{i=1}^{N} L(y_i,f(x_i))
$$

- 期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本集的平均损失。期望风险不能直接计算，常用经验风险估计期望风险。

- **经验风险最小化**

  - 经验风险最小化策略认为，经验风险最小的模型是最优的模型
    $$
    \min\limits_{f\in\mathcal{F}} \frac{1}{N} \sum\limits_{i=1}^{N} L(y_i,f(x_i))
    $$

  - 当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计

  - 当样本容量很小时，经验风险最小化学习的效果未必很好，会产生 **过拟合** 现象

- **结构风险最小化**

  - 结构风险最小化是为了防止过拟合提出的策略

  - 结构风险最小化等价于**正则化**

  - 结构风险在经验风险上加上表示模型复杂度的正则化项或惩罚项
    $$
    R_{\operatorname*{srm}}(f) = \frac{1}{N} \sum\limits_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
    $$

  - 结构风险最小化策略认为，结构风险最小的模型是最优的模型

- 结合经验风险最小化和结构风险最小化，监督学习就变成了经验风险或结构风险函数的最优化问题
  $$
  \min\limits_{f\in\mathcal{F}} \frac{1}{N}\sum\limits_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
  $$





## 算法

- 最后需要考虑的是用什么样的计算方法求解最优模型。这时，统计学习问题**归结为最优化问题**。


- 最优化问题有显式的**解析解**时，相对简单。但通常解析解不存在，就需要用**数值计算**的方法求解。
- 如何找到全局最优解，并使求解过程非常高效，就成了一个重要问题。

# 模型评估与模型选择

- 训练误差和测试误差是学习方法评估的标准
- 测试误差反映了学习方法对未知的测试数据集的预测能力
- 通常将学习方法对未知数据的预测能力称为**泛化能力**

# 正则化与交叉验证

- 正则化和交叉验证是两种常用的模型选择方法

## 正则化

- 正则化一般具有如下形式
  $$
  \min\limits_{f\in\mathcal{F}} \frac{1}{N}\sum\limits_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
  $$

- **正则化的作用是选择经验风险与模型复杂度同时较小的模型**

- 正则化符合 **奥卡姆剃刀** 原理

- 从贝叶斯估计的角度看，正则化项对应于模型的先验概率，可以假设

  - 复杂度模型有较小的先验概率
  - 简单的模型具有较大的先验概率

## 交叉验证

- 交叉验证的基本思想是 **重复地使用数据**
- 常用交叉验证方法
  - 简单交叉验证
  - S折交叉验证
  - 留一交叉验证

# 泛化能力

学习方法的**泛化能力**（generalization ability）是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。

- 通常通过测试误差来评价学习方法的泛化能力。
- 如果学到的模型 $\hat{f}$，那么用这个模型对未知数据预测的误差即为**泛化误差**（generalization error）。
  - 泛化误差反映了学习方法的泛化能力。
  - 事实上，泛化误差就是所学习到的模型的期望风险

# 生成模型与判别模型

监督学习的任务是学习一个模型，这个模型的一般形式为决策函数 $Y=f(X)$ 或条件概率分布 $P(Y|X)$。

监督学习方法又可以分为生成方法（generative approach）和判别方法（discriminative approach）。所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）。

## 生成模型

生成方法由数据学习的联合概率分布 $P(X|Y)​$ ，然后求出条件概率分布 $P(Y|X)​$ 作为预测的模型，即生成模型：
$$
P(Y|X) = \frac{P(X,Y)}{P(X)}
$$

这样的方法之所以称为生成方法，是因为模型表示了给定输入 $X$ 产生输出 $Y$ 的生成关系。典型的生成模型有：

- 朴素贝叶斯法
- 隐马尔可夫模型

## 判别模型

判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(Y|X)$ 作为预测的模型，即判别模型。

判别方法关心的是对给定的输入 $X$，应该预测什么样的输出 $Y$。典型的判别模型包括：

- $k$ 近邻法
- 感知机
- 决策树
- 逻辑斯蒂回归模型
- 最大熵模型
- 支持向量机
- 提升方法
- 条件随机场

## 对比

生成方法和判别方法各有优缺点，适合于不同条件下的学习问题。

**生成方法的特点**

- 生成方法可以还原出联合概率分布 $P(X,Y)$，而判别方法则不能
- 生成方法的学习收敛速度更快，即当样本容量增加的时候，学习的模型可以更快地收敛于真实模型
- 当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用

**判别方法的特点**

- 判别方法直接学习的是条件概率 $P(Y|X)$ 或决策函数 $f(X)$ ，直接面对预测，往往学习的准确率更高
- 由于直接学习 $P(Y|X)$ 或 $f(X)$ ，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题

# 监督学习几类问题

## 分类问题

- 当输出变量 $Y$ 取有限个离散值时，预测问题便成为分类问题
- 监督学习从数据中学习一个分类模型或分类决策函数，称为**分类器**（classifier），分类器对新的输入进行输出的**预测**（prediction），这个过程称为**分类**（classification）
- 可能的输出称为**类**（class）
- 评价分类器性能的指标一般是**分类准确率**（accuracy），其定义是：对于给定的数据集，分类器正确分类的样本数与样本数之比。也就是损失函数是 0-1 损失时测试数据集上的准确率
- 对于二分类问题，常用的评价指标是精确率（precision）与召回率（recall）
- 许多统计学习方法可以用于分类，包括
  - $k$ 近邻法
  - 感知机
  - 朴素贝叶斯法
  - 决策树
  - 决策列表
  - 逻辑斯蒂回归模型
  - 支持向量机
  - 提升方法
  - 贝叶斯网络
  - 神经网络
  - Winnow

## 标注问题

- 可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测问题的简单形式
- 标注问题的输入是一个观测序列，输出是一个标记序列或状态序列
- 标注问题的目标是学习一个模型，使它能够对观测序列给出标记序列作为预测
- 评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率、精确率和召回率
- 标记常用的统计方法有
  - 隐马尔可夫模型
  - 条件随机场
- 标注问题在信息提取、自然语言处理等领域被广泛应用

## 回归问题

- 回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化
- 回归模型正是表示从输入变量到输出变量之间的映射函数
- 回归问题等价于**函数拟合**：选择一条函数曲线使其很好地拟合已知数据，且很好地预测未知数据
- 回归问题按照输入变量的个数，分为一元回归和多元回归
- 按照输入变量和输出变量之间的关系，分为线性回归和非线性回归
- 回归学习最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法求解

